# GenAI--WandB-Evaluation-and-Debugging
Evaluating programs utilizing LLMS and Generative Image Models using platform-independent tools

# ğŸ¤– Generative AI- WandB: Evaluation and Debugging

In this repo, we'll evaluate programs utilizing LLMS and Generative Image Models using platform-independent tools, instrument a trainung notebook (add tracking, versioning, and logging), and implement monitoring/tracing of LLMs over time in complex interactions.

We'll use the Weights & Biases (WandB) to track experiments and run and version data. We'll have a semantic workflow to boost productivity. 

### âš™ï¸The colab link to the code is found and (will also be included in this repo) [here.](https://colab.research.google.com/drive/1OR0dpmer4AFPwvKvA3e5dW8C_lU8o2D3?usp=sharing)

# âš™ï¸The Build Process (Deliverables)

### Build ğŸ—ï¸
* Instrument a Jupyter notebook
* Manage hyperparameter config
* Log run metrics
* Collect artifacts for dataset and model versioning
* Log experiment results
* Trace prompts and responses to LLMs over time in complex interactions
